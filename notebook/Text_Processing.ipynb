{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Processing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0y2BL4zkLrZ"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlFkaU2Akb5M"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O2iy_i9mcoW",
        "outputId": "47eda32f-da80-42ae-d882-f8b469c9ba07"
      },
      "source": [
        "#tokenization\n",
        "sentence = nlp(u'\"They\\'re leaving U.K. for U.S.A.\"')\n",
        "for word in sentence:\n",
        "  print(word)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"\n",
            "They\n",
            "'re\n",
            "leaving\n",
            "U.K.\n",
            "for\n",
            "U.S.A.\n",
            "\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiVixa0Tmv0e",
        "outputId": "f5d8cc49-aa26-442f-d3d8-58e242cec169"
      },
      "source": [
        "#tokenization\n",
        "sentence1 = nlp(u\"Hello, I am non-vegetarian, email me the menu at abc-xyz@gmai.com\")\n",
        "for word in sentence1:\n",
        "  print(word)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello\n",
            ",\n",
            "I\n",
            "am\n",
            "non\n",
            "-\n",
            "vegetarian\n",
            ",\n",
            "email\n",
            "me\n",
            "the\n",
            "menu\n",
            "at\n",
            "abc-xyz@gmai.com\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IPcaWPlkczj",
        "outputId": "695a3b76-0a07-48bc-8f2f-4936a895ffac"
      },
      "source": [
        "#sentence segmentation\n",
        "doc = nlp(u\"I Love U.S.A. I've been living in N.Y.C. for more than 10 years now.\")\n",
        "for sent in doc.sents:\n",
        "  print(sent)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I Love U.S.A.\n",
            "I've been living in N.Y.C. for more than 10 years now.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYtnzck6oFe8",
        "outputId": "4ce2b0d8-97f0-4687-8bb7-7351946bf7e1"
      },
      "source": [
        "#lemmatization\n",
        "sentence2 = nlp(u'A letter has been written, asking him to be released')\n",
        "for word in sentence2:\n",
        "    print(word.text + '  ===>', word.lemma_)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A  ===> a\n",
            "letter  ===> letter\n",
            "has  ===> have\n",
            "been  ===> be\n",
            "written  ===> write\n",
            ",  ===> ,\n",
            "asking  ===> ask\n",
            "him  ===> -PRON-\n",
            "to  ===> to\n",
            "be  ===> be\n",
            "released  ===> release\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBAWylU6rFjs",
        "outputId": "806a4e14-9e91-4b2c-9592-d580b8891967"
      },
      "source": [
        "#stemming\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "tokens = ['compute', 'computer', 'computed', 'computing']\n",
        "for token in tokens:\n",
        "    print(token + ' --> ' + stemmer.stem(token))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "compute --> comput\n",
            "computer --> comput\n",
            "computed --> comput\n",
            "computing --> comput\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TTGg0cTrVOM",
        "outputId": "97dd445d-6592-43f1-f48a-46b9fda1d9d8"
      },
      "source": [
        "#lemmatization\n",
        "sentence3 = nlp(u'compute computer computed computing')\n",
        "for word in sentence3:\n",
        "    print(word.text + '  ===>', word.lemma_)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "compute  ===> compute\n",
            "computer  ===> computer\n",
            "computed  ===> compute\n",
            "computing  ===> computing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRC-OH_lr1JG"
      },
      "source": [
        "#lemmatization\n",
        "https://cst.dk/tools/index.php"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1l1eaqKvh5m",
        "outputId": "8b2bf17c-f6eb-49fe-b017-025851fa4adb"
      },
      "source": [
        "!python -m spacy download pt_core_news_sm"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pt_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz (21.2MB)\n",
            "\u001b[K     |████████████████████████████████| 21.2MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from pt_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (56.1.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.4.1)\n",
            "Building wheels for collected packages: pt-core-news-sm\n",
            "  Building wheel for pt-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pt-core-news-sm: filename=pt_core_news_sm-2.2.5-cp37-none-any.whl size=21186283 sha256=0cd8f929ae5ad091f6e5f2a0ab19c183b25817b64df8dc38f4ea0898d5ecc619\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z5pvrmxm/wheels/ea/94/74/ec9be8418e9231b471be5dc7e1b45dd670019a376a6b5bc1c0\n",
            "Successfully built pt-core-news-sm\n",
            "Installing collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('pt_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMd766uVu0ro"
      },
      "source": [
        "import spacy\n",
        "nlp1 = spacy.load('pt_core_news_sm')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNkeqZWrvv-0",
        "outputId": "f02a3142-0a2a-40e9-80d9-047a09924e4f"
      },
      "source": [
        "sentence4 = nlp1(u'São Paulo é a cidade mais populosa do Brasil.')\n",
        "for word in sentence4:\n",
        "    print(word.text + '  ===>', word.lemma_)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "São  ===> São\n",
            "Paulo  ===> Paulo\n",
            "é  ===> ser\n",
            "a  ===> o\n",
            "cidade  ===> cidade\n",
            "mais  ===> mais\n",
            "populosa  ===> populoso\n",
            "do  ===> do\n",
            "Brasil  ===> Brasil\n",
            ".  ===> .\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}