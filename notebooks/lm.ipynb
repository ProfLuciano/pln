{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC1s548gfBwq",
        "outputId": "ef986bb9-329f-44be-f40b-5196ff5322af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#perplexity\n",
        "import nltk\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm import Vocabulary\n",
        "\n",
        "train_sentences = ['an apple', 'an orange', 'the green apple is sweet']\n",
        "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) for sent in train_sentences]\n",
        "\n",
        "n = 2\n",
        "train_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
        "words = [word for sent in tokenized_text for word in sent]\n",
        "words.extend([\"<s>\", \"</s>\"])\n",
        "padded_vocab = Vocabulary(words)\n",
        "model = MLE(n)\n",
        "model.fit(train_data, padded_vocab)\n",
        "\n",
        "test_sentences = ['an apple', 'an ant']\n",
        "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) for sent in test_sentences]\n",
        "\n",
        "test_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
        "for test in test_data:\n",
        "    print (\"MLE Estimates:\", [((ngram[-1], ngram[:-1]),model.score(ngram[-1], ngram[:-1])) for ngram in test])\n",
        "\n",
        "test_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
        "for i, test in enumerate(test_data):\n",
        "  print(\"PP({0}):{1}\".format(test_sentences[i], model.perplexity(test)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLE Estimates: [(('an', ('<s>',)), 0.6666666666666666), (('apple', ('an',)), 0.5), (('</s>', ('apple',)), 0.5)]\n",
            "MLE Estimates: [(('an', ('<s>',)), 0.6666666666666666), (('ant', ('an',)), 0.0), (('</s>', ('ant',)), 0)]\n",
            "PP(an apple):1.8171205928321397\n",
            "PP(an ant):inf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gTfdRM5dFYD",
        "outputId": "1a684fc9-6024-486b-cbec-8086673ef7d0"
      },
      "source": [
        "!git clone https://github.com/IBM/deep-learning-language-model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning-language-model'...\n",
            "remote: Enumerating objects: 395, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 395 (delta 0), reused 0 (delta 0), pack-reused 393\u001b[K\n",
            "Receiving objects: 100% (395/395), 37.74 MiB | 24.97 MiB/s, done.\n",
            "Resolving deltas: 100% (185/185), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-TFPy1fdjdK"
      },
      "source": [
        "from __future__ import print_function\n",
        "import json\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import requests\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFkSck_1eUcb",
        "outputId": "602aa4ba-23cd-47bc-c2d2-9c997247cd86"
      },
      "source": [
        "path = 'deep-learning-language-model/yelp_100_3.txt'\n",
        "text = open(path).read().lower()\n",
        "print('corpus length:', len(text))\n",
        "char_indices = json.loads(open('deep-learning-language-model/char_indices.txt').read())\n",
        "indices_char = json.loads(open('deep-learning-language-model/indices_char.txt').read())\n",
        "chars = sorted(char_indices.keys())\n",
        "print(indices_char)\n",
        "#chars = sorted(list(set(text)))\n",
        "print('total chars:', len(chars))\n",
        "#char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "#indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 256\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n",
        "\n",
        "print('Vectorization...')\n",
        "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "\n",
        "# build the model: a single LSTM\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(LSTM(1024, return_sequences=True, input_shape=(maxlen, len(chars))))\n",
        "model.add(LSTM(512, return_sequences=False))\n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "optimizer = Adam(lr=0.002)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "model.load_weights(\"deep-learning-language-model/transfer_weights\")\n",
        "\n",
        "def sample(preds, temperature=.6):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "# train the model, output generated text after each iteration\n",
        "for iteration in range(1, 60):\n",
        "    print()\n",
        "    print('-' * 50)\n",
        "    print('Iteration', iteration)\n",
        "    x = np.zeros((1, maxlen, len(chars)))\n",
        "    preds = model.predict(x, verbose=0)[0]\n",
        "    \n",
        "    model.fit(X, y, batch_size=128, epochs=1)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    #start_index = char_indices[\"{\"]\n",
        "\n",
        "    for diversity in [0.2, 0.4, 0.6, 0.8]:\n",
        "        print()\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "        for i in range(400):\n",
        "            x = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            #print(next_index)\n",
        "            #print (indices_char)\n",
        "            next_char = indices_char[str(next_index)]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "model.save_weights(\"transfer_weights\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus length: 71250\n",
            "{'0': '\\n', '1': ' ', '2': '!', '3': '\"', '4': '#', '5': '$', '6': '%', '7': '&', '8': \"'\", '9': '(', '10': ')', '11': '*', '12': '+', '13': ',', '14': '-', '15': '.', '16': '/', '17': '0', '18': '1', '19': '2', '20': '3', '21': '4', '22': '5', '23': '6', '24': '7', '25': '8', '26': '9', '27': ':', '28': ';', '29': '=', '30': '?', '31': '[', '32': ']', '33': 'a', '34': 'b', '35': 'c', '36': 'd', '37': 'e', '38': 'f', '39': 'g', '40': 'h', '41': 'i', '42': 'j', '43': 'k', '44': 'l', '45': 'm', '46': 'n', '47': 'o', '48': 'p', '49': 'q', '50': 'r', '51': 's', '52': 't', '53': 'u', '54': 'v', '55': 'w', '56': 'x', '57': 'y', '58': 'z', '59': '{', '60': '}'}\n",
            "total chars: 61\n",
            "nb sequences: 23665\n",
            "Vectorization...\n",
            "Build model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Iteration 1\n",
            "185/185 [==============================] - 5041s 27s/step - loss: 1.4968\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"e this place, or at least like it.  we walked out of there terribly disappointed, i doubt we'll go back.}{this is a nothing but pies place...so i was expecting a little bit more oomph...\n",
            "\n",
            "dispointing small selection of pie by the slices...also you gotta pu\"\n",
            "e this place, or at least like it.  we walked out of there terribly disappointed, i doubt we'll go back.}{this is a nothing but pies place...so i was expecting a little bit more oomph...\n",
            "\n",
            "dispointing small selection of pie by the slices...also you gotta pup and the service is no the compery to spe is a little shopp of the salad, and i have to compery to seated a need a bit of meal for me some cheeses of the compery chicken was a bit of perfect for me hearthele for delicious are or meal with out the cook torato salad.  i would be back and salad and the service was so it was awayone.  i have been at the porking and the patio soon.  it's a bit of perf\n",
            "\n",
            "----- diversity: 0.4\n",
            "----- Generating with seed: \"e this place, or at least like it.  we walked out of there terribly disappointed, i doubt we'll go back.}{this is a nothing but pies place...so i was expecting a little bit more oomph...\n",
            "\n",
            "dispointing small selection of pie by the slices...also you gotta pu\"\n",
            "e this place, or at least like it.  we walked out of there terribly disappointed, i doubt we'll go back.}{this is a nothing but pies place...so i was expecting a little bit more oomph...\n",
            "\n",
            "dispointing small selection of pie by the slices...also you gotta pup to order there list aid to semperately definitely complained a little soup bowt or no salad.  i read a bit of combo and i tas a be a bit are served that the compery cours after the food and some definitely for a cheese of the food and the portious are patio seeped to the were so som the cook a bar came every the salad.  i wish a they were so the salad was good.  i would i and a beener with a she\n",
            "\n",
            "----- diversity: 0.6\n",
            "----- Generating with seed: \"e this place, or at least like it.  we walked out of there terribly disappointed, i doubt we'll go back.}{this is a nothing but pies place...so i was expecting a little bit more oomph...\n",
            "\n",
            "dispointing small selection of pie by the slices...also you gotta pu\"\n",
            "e this place, or at least like it.  we walked out of there terribly disappointed, i doubt we'll go back.}{this is a nothing but pies place...so i was expecting a little bit more oomph...\n",
            "\n",
            "dispointing small selection of pie by the slices...also you gotta pupked cook ride arearized carnet sormed list order and was pretty good... like the place for the service was to it would resure.  i could remember ne soup really cooked and the best pie partake of can't be peep care asters aithe food is a fremint stopping onerow with our tacking and i was their supprinted and some more salad.  it's not see with they come but the service is that it is so the tained \n",
            "\n",
            "----- diversity: 0.8\n",
            "----- Generating with seed: \"e this place, or at least like it.  we walked out of there terribly disappointed, i doubt we'll go back.}{this is a nothing but pies place...so i was expecting a little bit more oomph...\n",
            "\n",
            "dispointing small selection of pie by the slices...also you gotta pu\"\n",
            "e this place, or at least like it.  we walked out of there terribly disappointed, i doubt we'll go back.}{this is a nothing but pies place...so i was expecting a little bit more oomph...\n",
            "\n",
            "dispointing small selection of pie by the slices...also you gotta pulled bit pia chacked meat right and everything it to my fresh, but the food was suria and the more caperian the mell. the place, are presty zeasone of the charring for ence.\n",
            " in the service was the sit eef like thai ovenome....oin cames and have been then it was awayone of it would be couved that i love the selped with somporation. i'd uninately dees and defenotely at the boytle with place. the ir\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 2\n",
            "185/185 [==============================] - 5085s 27s/step - loss: 1.0459\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"m has was way better.  if you go here.... order a burger.  they are big, juicy and super duper good.  it's also super duper messy.  it's a \"fork and knife\" type of burger. \n",
            "\n",
            "their bloody mary is pretty good... a little too tomato-y.... needed more seasonin\"\n",
            "m has was way better.  if you go here.... order a burger.  they are big, juicy and super duper good.  it's also super duper messy.  it's a \"fork and knife\" type of burger. \n",
            "\n",
            "their bloody mary is pretty good... a little too tomato-y.... needed more seasoning of my was got the blabbend than the best pista was good. i don't thing that is that the bonther for moss of the coffee serve"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "r was potato for a biers and difen anout the best it wasn't and shoppen and the corner of that is a girling that that is that the staff with cashes and the borther and she then the best it wasn't that the best pista was got the best past on the menu and she is a great place\n",
            "\n",
            "----- diversity: 0.4\n",
            "----- Generating with seed: \"m has was way better.  if you go here.... order a burger.  they are big, juicy and super duper good.  it's also super duper messy.  it's a \"fork and knife\" type of burger. \n",
            "\n",
            "their bloody mary is pretty good... a little too tomato-y.... needed more seasonin\"\n",
            "m has was way better.  if you go here.... order a burger.  they are big, juicy and super duper good.  it's also super duper messy.  it's a \"fork and knife\" type of burger. \n",
            "\n",
            "their bloody mary is pretty good... a little too tomato-y.... needed more seasoning that when i saided tort out the change that is a great staff.\n",
            "\n",
            "the salad and was so my friends and the most of the copped food coffee of pretis souss of peryonia and she was going to get the best past one of the strained had a but in back.  i readly did not seem and i was darg. the salad then i had a bit and served the place is no my crandlers and the best it is a good light atarle of that is to\n",
            "\n",
            "----- diversity: 0.6\n",
            "----- Generating with seed: \"m has was way better.  if you go here.... order a burger.  they are big, juicy and super duper good.  it's also super duper messy.  it's a \"fork and knife\" type of burger. \n",
            "\n",
            "their bloody mary is pretty good... a little too tomato-y.... needed more seasonin\"\n",
            "m has was way better.  if you go here.... order a burger.  they are big, juicy and super duper good.  it's also super duper messy.  it's a \"fork and knife\" type of burger. \n",
            "\n",
            "their bloody mary is pretty good... a little too tomato-y.... needed more seasoning so i as topting that the kquest of the food as the coster and the gring that is a greated in the brusscot as worth dish was very drinks and the casual blead of that was firited and shoppen mine plate to sle and drinks and everyone that the sit is got thene was a bit of cozbor about town of every was good.  i readly good. i don't thing to get a sure white was green and carnet boos....one thing we\n",
            "\n",
            "----- diversity: 0.8\n",
            "----- Generating with seed: \"m has was way better.  if you go here.... order a burger.  they are big, juicy and super duper good.  it's also super duper messy.  it's a \"fork and knife\" type of burger. \n",
            "\n",
            "their bloody mary is pretty good... a little too tomato-y.... needed more seasonin\"\n",
            "m has was way better.  if you go here.... order a burger.  they are big, juicy and super duper good.  it's also super duper messy.  it's a \"fork and knife\" type of burger. \n",
            "\n",
            "their bloody mary is pretty good... a little too tomato-y.... needed more seasoning and the dishes that not the but that it is the mesued point and sardation.\n",
            "\n",
            "we will we wasne restaurant food.  i can't do a few the other cringe of many red cheese in the burgers and then it to stals just liked thai castem dried thanks heart hed for for calioss but i tas a bit of burgers mide the castua cheff... i was avouade the strained cricken and tasty, sauce. the corterion. the bnooded towe\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 3\n",
            "185/185 [==============================] - 5061s 27s/step - loss: 0.8124\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"n't go wrong with a meal from flemings. they also have a great happy hour.}{ok, first i broke my promise to myself and went here again, i figured by now things have got to be better. i ordered something that i wouldn't care if it was cold, french toast. ok\"\n",
            "n't go wrong with a meal from flemings. they also have a great happy hour.}{ok, first i broke my promise to myself and went here again, i figured by now things have got to be better. i ordered something that i wouldn't care if it was cold, french toast. okey mext on it was great!}{i was great!}{i was great!}{i was great!}{of pluce for the bottood the concert in the mood and i sandwich carner back and the bottle of salad.  i would be town the service was completely bottle the best say we were coffee food, but the worth stuffed bottles, but it's not sure the but in a try morn reliming the chicken was great!}{i was great!}{i was gream!}{one of the bes\n",
            "\n",
            "----- diversity: 0.4\n",
            "----- Generating with seed: \"n't go wrong with a meal from flemings. they also have a great happy hour.}{ok, first i broke my promise to myself and went here again, i figured by now things have got to be better. i ordered something that i wouldn't care if it was cold, french toast. ok\"\n",
            "n't go wrong with a meal from flemings. they also have a great happy hour.}{ok, first i broke my promise to myself and went here again, i figured by now things have got to be better. i ordered something that i wouldn't care if it was cold, french toast. okey the guy  the but it was great!}{i was great!}{i really like the concert to stuffer heart selection, but the bottor in the month time mare there was all while the food was colled to give the food as well, but the but for the stuff time ther best start.}{the pork sausage there was sublen with the best say i was great!}{i got a breakfast nucct. my house and we giff the food was gond regunder and t\n",
            "\n",
            "----- diversity: 0.6\n",
            "----- Generating with seed: \"n't go wrong with a meal from flemings. they also have a great happy hour.}{ok, first i broke my promise to myself and went here again, i figured by now things have got to be better. i ordered something that i wouldn't care if it was cold, french toast. ok\"\n",
            "n't go wrong with a meal from flemings. they also have a great happy hour.}{ok, first i broke my promise to myself and went here again, i figured by now things have got to be better. i ordered something that i wouldn't care if it was cold, french toast. okey, i tasted, i loce to stuple but you can tho the went time to get my lunch and hot smoke in a day was salad.  i got not in non a friday.  the case was sorn food, but the bottood there were some cooked to get it line when the borther.  the ghing the coffee cool and there and the tasted mext and strowge to gin bat bry on our server who the concert of ited to git was very place? for least it wouldn\n",
            "\n",
            "----- diversity: 0.8\n",
            "----- Generating with seed: \"n't go wrong with a meal from flemings. they also have a great happy hour.}{ok, first i broke my promise to myself and went here again, i figured by now things have got to be better. i ordered something that i wouldn't care if it was cold, french toast. ok\"\n",
            "n't go wrong with a meal from flemings. they also have a great happy hour.}{ok, first i broke my promise to myself and went here again, i figured by now things have got to be better. i ordered something that i wouldn't care if it was cold, french toast. okey like the who is good.  i got not smont the only of it's not light on it was gond, they difference fus breakfast/\n",
            "\n",
            "                   which i think it to as been. the orders spot in, and i love them compored at left op like the checry, but it was relling of spring, but for a like plais, but the butter than now the outsill and the ones bet sour waitress and the one of them sour not but bows of pa\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 4\n",
            "185/185 [==============================] - 4974s 27s/step - loss: 0.6294\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"k; people coming in for happy hour (jeans and nice shirts); and people just in off the street (shorts, casual shirts, even hats). i really liked this ambiance, because it doesn't make it too stuffy and turn people \"off\" of this locally grown, organic resta\"\n",
            "k; people coming in for happy hour (jeans and nice shirts); and people just in off the street (shorts, casual shirts, even hats). i really liked this ambiance, because it doesn't make it too stuffy and turn people \"off\" of this locally grown, organic restaurant with conerelian, but awally a good pasty, cheese, be a good pattion, and to fined to my coangrels and the food was weined a nice for the mood at the booth.  i don't think it's a bit spicy compleint for the mood to stuffed heart see ed the other to the amme. their head giver the best pay i hon a higkend's carnet as a good place for the orders the food and to the took time.  he whone their hea\n",
            "\n",
            "----- diversity: 0.4\n",
            "----- Generating with seed: \"k; people coming in for happy hour (jeans and nice shirts); and people just in off the street (shorts, casual shirts, even hats). i really liked this ambiance, because it doesn't make it too stuffy and turn people \"off\" of this locally grown, organic resta\"\n",
            "k; people coming in for happy hour (jeans and nice shirts); and people just in off the street (shorts, casual shirts, even hats). i really liked this ambiance, because it doesn't make it too stuffy and turn people \"off\" of this locally grown, organic restaurant with comerearing, but he was our the food as the boythere and the booth.  they are nead by night we ordered one of the restaurant with corneration ene at the boy new cook day niget a few the ingo arsounctuace.  your tables icristeak--\n",
            "\n",
            "am into the spreing and the sauce.  i've had been agreed the salad great sitsing out our monuts, we did dot real deed the salad day was good. it was going to \n",
            "\n",
            "----- diversity: 0.6\n",
            "----- Generating with seed: \"k; people coming in for happy hour (jeans and nice shirts); and people just in off the street (shorts, casual shirts, even hats). i really liked this ambiance, because it doesn't make it too stuffy and turn people \"off\" of this locally grown, organic resta\"\n",
            "k; people coming in for happy hour (jeans and nice shirts); and people just in off the street (shorts, casual shirts, even hats). i really liked this ambiance, because it doesn't make it too stuffy and turn people \"off\" of this locally grown, organic restaurant with the salfed a gere at bemoration. the counter was smonting and had the best phener agoun to tasty, and to me...all who had oy the botter on a fan brunched and to the took tite.  they have had it's a bit conside.  ay i'm keeat beer fart the best restaurant with the sarm the cheese to salad nead a keep better picky with dinner about did tot sure the and takes for the when i have a too hone\n",
            "\n",
            "----- diversity: 0.8\n",
            "----- Generating with seed: \"k; people coming in for happy hour (jeans and nice shirts); and people just in off the street (shorts, casual shirts, even hats). i really liked this ambiance, because it doesn't make it too stuffy and turn people \"off\" of this locally grown, organic resta\"\n",
            "k; people coming in for happy hour (jeans and nice shirts); and people just in off the street (shorts, casual shirts, even hats). i really liked this ambiance, because it doesn't make it too stuffy and turn people \"off\" of this locally grown, organic restaurant was their asten hings to music out on a sandwich was \"comporedions the best pay so it's a givinite (head of side.  i thought the heart delest to salatt. the dispecation. if you and they good the mix people's like.  the oldiods to the good tith and delicious.  our vegrees food.}{i read for our in the morning free is my rita and another con spot to mothough for the bouturd. and the coon tastic\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 5\n",
            " 75/185 [===========>..................] - ETA: 49:10 - loss: 0.4308"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQpo9THFffJV",
        "outputId": "e313398d-90de-408e-df03-c6c0a23edce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "source": [
        "!pip install nltk==3.5"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\r\u001b[K     |▎                               | 10kB 20.4MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 28.7MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 31.1MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 24.7MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 15.4MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 12.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 13.8MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 15.2MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 15.9MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 16.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 16.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 16.1MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 16.1MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 16.1MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 16.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 16.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 16.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 184kB 16.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 194kB 16.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 204kB 16.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 215kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 235kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 245kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 256kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 266kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 276kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 286kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 296kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 307kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 327kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 337kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 348kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 368kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 378kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 389kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 399kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 409kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 419kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 430kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 440kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 460kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 471kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 481kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 491kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 501kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 512kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 522kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 532kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 542kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 552kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 563kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 573kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 583kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 593kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 604kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 614kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 624kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 634kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 645kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 655kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 665kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 675kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 686kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 696kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 706kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 716kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 727kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 737kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 747kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 757kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 768kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 778kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 788kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 798kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 808kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 819kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 829kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 839kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 849kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 860kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 870kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 880kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 890kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 901kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 911kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 921kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 931kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 942kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 952kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 962kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 972kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 983kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 993kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.0MB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.0MB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0MB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.0MB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1MB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.1MB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.1MB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1MB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.1MB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.1MB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.2MB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.2MB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.2MB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.2MB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.2MB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2MB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.2MB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3MB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3MB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.3MB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.3MB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.3MB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.3MB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.3MB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3MB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.4MB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.4MB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.4MB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.4MB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.4MB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.4MB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.4MB 16.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 16.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (1.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (4.41.1)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp37-none-any.whl size=1434697 sha256=10c214a2fc770621468d990cecfedd56d5d45a5ad70d0467ba402a5929ea29d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IPI4mIDfqBJ",
        "outputId": "669f578a-1bc6-411b-a012-260442d22078",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('all')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    }
  ]
}