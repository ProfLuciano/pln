{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "character_rnn_for_ner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XNJRcmoIJQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08edc142-0f89-4b8b-d21f-c8358879add9"
      },
      "source": [
        "#http://alexminnaar.com/2019/08/22/ner-rnns-tensorflow.html\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTbafS33A2Z0",
        "outputId": "6b5d72e6-44c9-458e-a6be-ec06f690f36a"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/davidsbatista/NER-datasets/master/CONLL2003/test.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-26 12:00:05--  https://raw.githubusercontent.com/davidsbatista/NER-datasets/master/CONLL2003/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 748093 (731K) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>] 730.56K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-26 12:00:06 (6.06 MB/s) - ‘test.txt’ saved [748093/748093]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6hbTSZzBABg",
        "outputId": "8d48b4a0-13b1-4661-8c06-7604e072e23c"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/davidsbatista/NER-datasets/master/CONLL2003/train.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-26 12:00:11--  https://raw.githubusercontent.com/davidsbatista/NER-datasets/master/CONLL2003/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3283418 (3.1M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>]   3.13M  10.4MB/s    in 0.3s    \n",
            "\n",
            "2021-05-26 12:00:12 (10.4 MB/s) - ‘train.txt’ saved [3283418/3283418]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qmbgySVBESF",
        "outputId": "889db194-c6f2-4a2c-cd7d-e6619b50c04f"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/davidsbatista/NER-datasets/master/CONLL2003/valid.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-26 12:00:16--  https://raw.githubusercontent.com/davidsbatista/NER-datasets/master/CONLL2003/valid.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 827441 (808K) [text/plain]\n",
            "Saving to: ‘valid.txt’\n",
            "\n",
            "valid.txt           100%[===================>] 808.05K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-26 12:00:17 (6.26 MB/s) - ‘valid.txt’ saved [827441/827441]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chLTt1VrIa2s"
      },
      "source": [
        "labels = set()\n",
        "\n",
        "def file2Examples(file_name):\n",
        "  '''\n",
        "  Read data files and return input/output pairs\n",
        "  '''\n",
        "  \n",
        "  examples=[]\n",
        "\n",
        "  with open(file_name,\"r\") as f:\n",
        "\n",
        "    next(f)\n",
        "    next(f)\n",
        "\n",
        "    example = [[],[]]\n",
        "\n",
        "    for line in f:\n",
        "\n",
        "      input_output_split= line.split()\n",
        "\n",
        "      if len(input_output_split)==4:\n",
        "        example[0].append(input_output_split[0])\n",
        "        example[1].append(input_output_split[-1])\n",
        "        labels.add(input_output_split[-1])\n",
        "\n",
        "      elif len(input_output_split)==0:\n",
        "        examples.append(example)\n",
        "        example=[[],[]]\n",
        "      else:\n",
        "        example=[[],[]]\n",
        "\n",
        "    f.close()\n",
        "    \n",
        "    return examples\n",
        "  \n",
        "# Extract examples from train, validation, and test files which can be found at \n",
        "# https://github.com/davidsbatista/NER-datasets/tree/master/CONLL2003\n",
        "train_examples = file2Examples(\"train.txt\")\n",
        "test_examples = file2Examples(\"test.txt\")\n",
        "valid_examples = file2Examples(\"valid.txt\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2JQ22IHFfyQ",
        "outputId": "e2563911-b340-47a5-b334-b28e9c0232f2"
      },
      "source": [
        "!head train.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-DOCSTART- -X- -X- O\n",
            "\n",
            "EU NNP B-NP B-ORG\n",
            "rejects VBZ B-VP O\n",
            "German JJ B-NP B-MISC\n",
            "call NN I-NP O\n",
            "to TO B-VP O\n",
            "boycott VB I-VP O\n",
            "British JJ B-NP B-MISC\n",
            "lamb NN I-NP O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of8tTTetJFAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5957cc9-3a1d-4fbb-a2ad-07cad0115137"
      },
      "source": [
        "    # create character vocab\n",
        "    all_text = \" \".join([\" \".join(x[0]) for x in train_examples+valid_examples+test_examples])\n",
        "    vocab = sorted(set(all_text))\n",
        "    \n",
        "    # create character/id and label/id mapping\n",
        "    char2idx = {u:i+1 for i, u in enumerate(vocab)}\n",
        "    idx2char = np.array(vocab)\n",
        "    label2idx = {u:i+1 for i, u in enumerate(labels)}\n",
        "    idx2label = np.array(labels)\n",
        "    \n",
        "    print(idx2label)\n",
        "    print(char2idx)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-ORG', 'I-ORG', 'I-MISC', 'B-MISC', 'B-LOC', 'O', 'B-PER', 'I-PER', 'I-LOC'}\n",
            "{' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, '+': 12, ',': 13, '-': 14, '.': 15, '/': 16, '0': 17, '1': 18, '2': 19, '3': 20, '4': 21, '5': 22, '6': 23, '7': 24, '8': 25, '9': 26, ':': 27, ';': 28, '=': 29, '?': 30, '@': 31, 'A': 32, 'B': 33, 'C': 34, 'D': 35, 'E': 36, 'F': 37, 'G': 38, 'H': 39, 'I': 40, 'J': 41, 'K': 42, 'L': 43, 'M': 44, 'N': 45, 'O': 46, 'P': 47, 'Q': 48, 'R': 49, 'S': 50, 'T': 51, 'U': 52, 'V': 53, 'W': 54, 'X': 55, 'Y': 56, 'Z': 57, '[': 58, ']': 59, '`': 60, 'a': 61, 'b': 62, 'c': 63, 'd': 64, 'e': 65, 'f': 66, 'g': 67, 'h': 68, 'i': 69, 'j': 70, 'k': 71, 'l': 72, 'm': 73, 'n': 74, 'o': 75, 'p': 76, 'q': 77, 'r': 78, 's': 79, 't': 80, 'u': 81, 'v': 82, 'w': 83, 'x': 84, 'y': 85, 'z': 86}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHtLQrq4JhJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "601ad47c-d108-4690-ccb1-c663f785ef3e"
      },
      "source": [
        "    def split_char_labels(eg):\n",
        "      '''\n",
        "      For a given input/output example, break tokens into characters while keeping \n",
        "      the same label.\n",
        "      '''\n",
        "\n",
        "      tokens = eg[0]\n",
        "      labels=eg[1]\n",
        "\n",
        "      input_chars = []\n",
        "      output_char_labels = []\n",
        "\n",
        "      for token,label in zip(tokens,labels):\n",
        "\n",
        "        input_chars.extend([char for char in token])\n",
        "        input_chars.extend(' ')\n",
        "        output_char_labels.extend([label]*len(token))\n",
        "        output_char_labels.extend('O')\n",
        "\n",
        "      return [[char2idx[x] for x in input_chars[:-1]],np.array([label2idx[x] for x in output_char_labels[:-1]])]\n",
        "   \n",
        "    train_formatted = [split_char_labels(eg) for eg in train_examples]\n",
        "    test_formatted = [split_char_labels(eg) for eg in test_examples]\n",
        "    valid_formatted = [split_char_labels(eg) for eg in valid_examples]\n",
        "    \n",
        "    print(len(train_formatted))\n",
        "    print(len(test_formatted))\n",
        "    print(len(valid_formatted))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14985\n",
            "3682\n",
            "3464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDRSAIObKBL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9fc4fd5-0112-4d68-ced6-c8d2755b3086"
      },
      "source": [
        "    # training generator\n",
        "    def gen_train_series():\n",
        "\n",
        "        for eg in train_formatted:\n",
        "          yield eg[0],eg[1]\n",
        "    \n",
        "    # validation generator\n",
        "    def gen_valid_series():\n",
        "    \n",
        "       for eg in valid_formatted:\n",
        "          yield eg[0],eg[1]\n",
        "    \n",
        "    # test generator\n",
        "    def gen_test_series():\n",
        "\n",
        "      for eg in test_formatted:\n",
        "          yield eg[0],eg[1]\n",
        "      \n",
        "    # create Dataset objects for train, test and validation sets  \n",
        "    series = tf.data.Dataset.from_generator(gen_train_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
        "    series_valid = tf.data.Dataset.from_generator(gen_valid_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
        "    series_test = tf.data.Dataset.from_generator(gen_test_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
        "\n",
        "    BATCH_SIZE = 128\n",
        "    BUFFER_SIZE=1000\n",
        "    \n",
        "    # create padded batch series objects for train, test and validation sets\n",
        "    ds_series_batch = series.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
        "    ds_series_batch_valid = series_valid.padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
        "    ds_series_batch_test = series_test.padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
        "    \n",
        "    # print example batches\n",
        "    for input_example_batch, target_example_batch in ds_series_batch_valid.take(1):\n",
        "      print(input_example_batch)\n",
        "      print(target_example_batch)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[34 49 40 ...  0  0  0]\n",
            " [43 46 45 ...  0  0  0]\n",
            " [54 65 79 ...  0  0  0]\n",
            " ...\n",
            " [ 3  1 36 ...  0  0  0]\n",
            " [40 66  1 ...  0  0  0]\n",
            " [35 81 78 ...  0  0  0]], shape=(128, 228), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[6 6 6 ... 0 0 0]\n",
            " [5 5 5 ... 0 0 0]\n",
            " [4 4 4 ... 0 0 0]\n",
            " ...\n",
            " [6 6 6 ... 0 0 0]\n",
            " [6 6 6 ... 0 0 0]\n",
            " [7 7 7 ... 0 0 0]], shape=(128, 228), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4msLXztSJtqo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ebdfd30-9375-4f94-f874-b68433d4c932"
      },
      "source": [
        "  vocab_size = len(vocab)+1\n",
        "\n",
        "  # The embedding dimension\n",
        "  embedding_dim = 256\n",
        "\n",
        "  # Number of RNN units\n",
        "  rnn_units = 1024\n",
        "\n",
        "  label_size = len(labels)  \n",
        "  \n",
        "  # build LSTM model\n",
        "  def build_model(vocab_size,label_size, embedding_dim, rnn_units, batch_size):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None],mask_zero=True),\n",
        "            tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "            tf.keras.layers.Dense(label_size)\n",
        "            ])\n",
        "        return model\n",
        "\n",
        "  model = build_model(\n",
        "        vocab_size = len(vocab)+1,\n",
        "        label_size=len(labels)+1,\n",
        "        embedding_dim=embedding_dim,\n",
        "        rnn_units=rnn_units,\n",
        "        batch_size=BATCH_SIZE)\n",
        "\n",
        "  model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (128, None, 256)          22272     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (128, None, 1024)         5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (128, None, 10)           10250     \n",
            "=================================================================\n",
            "Total params: 5,279,498\n",
            "Trainable params: 5,279,498\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1vnxVFcK1Hk"
      },
      "source": [
        "    import os\n",
        "\n",
        "    # define loss function\n",
        "    def loss(labels, logits):\n",
        "        return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "    model.compile(optimizer='adam', loss=loss,metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "    # Directory where the checkpoints will be saved\n",
        "    checkpoint_dir = './training_checkpoints'\n",
        "    # Name of the checkpoint files\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "    checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_prefix,\n",
        "        save_weights_only=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CQ2I9UDK9ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc1aa774-89dd-43ee-d42c-a2398d72ae06"
      },
      "source": [
        "    EPOCHS=20\n",
        "    history = model.fit(ds_series_batch, epochs=EPOCHS, validation_data=ds_series_batch_valid,callbacks=[checkpoint_callback])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "117/117 [==============================] - 17s 146ms/step - loss: 0.0931 - sparse_categorical_accuracy: 0.8753 - val_loss: 0.0963 - val_sparse_categorical_accuracy: 0.8778\n",
            "Epoch 2/20\n",
            "117/117 [==============================] - 17s 146ms/step - loss: 0.0849 - sparse_categorical_accuracy: 0.8865 - val_loss: 0.0882 - val_sparse_categorical_accuracy: 0.8889\n",
            "Epoch 3/20\n",
            "117/117 [==============================] - 17s 147ms/step - loss: 0.0799 - sparse_categorical_accuracy: 0.8938 - val_loss: 0.0835 - val_sparse_categorical_accuracy: 0.8967\n",
            "Epoch 4/20\n",
            "117/117 [==============================] - 17s 146ms/step - loss: 0.0765 - sparse_categorical_accuracy: 0.8992 - val_loss: 0.0805 - val_sparse_categorical_accuracy: 0.8997\n",
            "Epoch 5/20\n",
            "117/117 [==============================] - 17s 146ms/step - loss: 0.0722 - sparse_categorical_accuracy: 0.9042 - val_loss: 0.0788 - val_sparse_categorical_accuracy: 0.9022\n",
            "Epoch 6/20\n",
            "117/117 [==============================] - 17s 147ms/step - loss: 0.0691 - sparse_categorical_accuracy: 0.9085 - val_loss: 0.0762 - val_sparse_categorical_accuracy: 0.9051\n",
            "Epoch 7/20\n",
            "117/117 [==============================] - 17s 146ms/step - loss: 0.0662 - sparse_categorical_accuracy: 0.9123 - val_loss: 0.0738 - val_sparse_categorical_accuracy: 0.9102\n",
            "Epoch 8/20\n",
            "117/117 [==============================] - 17s 147ms/step - loss: 0.0633 - sparse_categorical_accuracy: 0.9164 - val_loss: 0.0717 - val_sparse_categorical_accuracy: 0.9122\n",
            "Epoch 9/20\n",
            "117/117 [==============================] - 17s 146ms/step - loss: 0.0602 - sparse_categorical_accuracy: 0.9209 - val_loss: 0.0715 - val_sparse_categorical_accuracy: 0.9116\n",
            "Epoch 10/20\n",
            "117/117 [==============================] - 17s 146ms/step - loss: 0.0572 - sparse_categorical_accuracy: 0.9250 - val_loss: 0.0687 - val_sparse_categorical_accuracy: 0.9164\n",
            "Epoch 11/20\n",
            "117/117 [==============================] - 17s 146ms/step - loss: 0.0538 - sparse_categorical_accuracy: 0.9296 - val_loss: 0.0681 - val_sparse_categorical_accuracy: 0.9185\n",
            "Epoch 12/20\n",
            "117/117 [==============================] - 17s 146ms/step - loss: 0.0505 - sparse_categorical_accuracy: 0.9335 - val_loss: 0.0670 - val_sparse_categorical_accuracy: 0.9187\n",
            "Epoch 13/20\n",
            "117/117 [==============================] - 17s 147ms/step - loss: 0.0468 - sparse_categorical_accuracy: 0.9381 - val_loss: 0.0687 - val_sparse_categorical_accuracy: 0.9190\n",
            "Epoch 14/20\n",
            "117/117 [==============================] - 17s 146ms/step - loss: 0.0438 - sparse_categorical_accuracy: 0.9423 - val_loss: 0.0660 - val_sparse_categorical_accuracy: 0.9227\n",
            "Epoch 15/20\n",
            "117/117 [==============================] - 17s 146ms/step - loss: 0.0404 - sparse_categorical_accuracy: 0.9473 - val_loss: 0.0646 - val_sparse_categorical_accuracy: 0.9251\n",
            "Epoch 16/20\n",
            "117/117 [==============================] - 17s 146ms/step - loss: 0.0366 - sparse_categorical_accuracy: 0.9520 - val_loss: 0.0657 - val_sparse_categorical_accuracy: 0.9265\n",
            "Epoch 17/20\n",
            "117/117 [==============================] - 17s 146ms/step - loss: 0.0338 - sparse_categorical_accuracy: 0.9559 - val_loss: 0.0673 - val_sparse_categorical_accuracy: 0.9264\n",
            "Epoch 18/20\n",
            "117/117 [==============================] - 17s 146ms/step - loss: 0.0309 - sparse_categorical_accuracy: 0.9595 - val_loss: 0.0680 - val_sparse_categorical_accuracy: 0.9273\n",
            "Epoch 19/20\n",
            "117/117 [==============================] - 17s 146ms/step - loss: 0.0284 - sparse_categorical_accuracy: 0.9628 - val_loss: 0.0698 - val_sparse_categorical_accuracy: 0.9261\n",
            "Epoch 20/20\n",
            "117/117 [==============================] - 17s 147ms/step - loss: 0.0270 - sparse_categorical_accuracy: 0.9648 - val_loss: 0.0701 - val_sparse_categorical_accuracy: 0.9279\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Oc6PpNmj8nR",
        "outputId": "2a0d455d-4778-4f40-b701-845db44e301e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "preds = np.array([])\n",
        "y_trues= np.array([])\n",
        "\n",
        "# iterate through test set, make predictions based on trained model\n",
        "for input_example_batch, target_example_batch in ds_series_batch_test:\n",
        "\n",
        "  pred=model.predict_on_batch(input_example_batch)\n",
        "  pred_max=tf.argmax(tf.nn.softmax(pred),2).numpy().flatten()\n",
        "  y_true=target_example_batch.numpy().flatten()\n",
        "\n",
        "  preds=np.concatenate([preds,pred_max])\n",
        "  y_trues=np.concatenate([y_trues,y_true])\n",
        "\n",
        "# remove padding from evaluation\n",
        "remove_padding = [(p,y) for p,y in zip(preds,y_trues) if y!=0]\n",
        "\n",
        "r_p = [x[0] for x in remove_padding]\n",
        "r_t = [x[1] for x in remove_padding]\n",
        "\n",
        "# print confusion matrix and classification report\n",
        "print(confusion_matrix(r_p,r_t))\n",
        "print(classification_report(r_p,r_t))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  4212     79     29    602    826    584    579     15      8]\n",
            " [    64   3120    113     26     69    646     89    517    198]\n",
            " [    27    382    461     27     10    340     24    206    129]\n",
            " [   897     68     41   2413    984    526    371     41     13]\n",
            " [  2006     51      0    788   7100    727    738     13      8]\n",
            " [  1809    400    223    661    774 186850    937    184    124]\n",
            " [  1091     66      8    299    664    655   6285     42     14]\n",
            " [    53    318     64     17     22    120     24   6049     58]\n",
            " [     3    249     71      4      0     72     16    210    820]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.41      0.61      0.49      6934\n",
            "         2.0       0.66      0.64      0.65      4842\n",
            "         3.0       0.46      0.29      0.35      1606\n",
            "         4.0       0.50      0.45      0.47      5354\n",
            "         5.0       0.68      0.62      0.65     11431\n",
            "         6.0       0.98      0.97      0.98    191962\n",
            "         7.0       0.69      0.69      0.69      9124\n",
            "         8.0       0.83      0.90      0.86      6725\n",
            "         9.0       0.60      0.57      0.58      1445\n",
            "\n",
            "    accuracy                           0.91    239423\n",
            "   macro avg       0.65      0.64      0.64    239423\n",
            "weighted avg       0.91      0.91      0.91    239423\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}